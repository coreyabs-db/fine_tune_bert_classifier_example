{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7cad852-d722-4f1a-8c05-bd9c63830d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Preprocess documents\n",
    "\n",
    "Apply any transformations, such as parsing, cleansing, etc... to the loaded documents.\n",
    "\n",
    "Here we pre-tokenize and count the tokens as an example of a preprocessing step you may\n",
    "want to run either before training or inference. Other transformations could include\n",
    "filtering invalid documents, deduplicating documents, enriching with additional metadata,\n",
    "and many other transformations you may need to meet your business goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4b1606-4180-47d1-9e39-7f1c868b0502",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and setup"
    }
   },
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Iterator\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from delta import DeltaTable\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession, DataFrame, Window, Row\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d436d241-bcf7-4c80-b05a-571f7a2fbb34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Task parameters"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"hugging_face_id\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "hugging_face_id = dbutils.widgets.get(\"hugging_face_id\")\n",
    "\n",
    "assert catalog_name, \"catalog_name is required\"\n",
    "assert schema_name, \"schema_name is required\"\n",
    "assert hugging_face_id, \"hugging_face_id is required\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "source_table_name = \"yelp_reviews_bronze\"\n",
    "target_table_name = \"yelp_reviews_silver\"\n",
    "\n",
    "print(f\"catalog_name: {catalog_name}\")\n",
    "print(f\"schema_name: {schema_name}\")\n",
    "print(f\"hugging_face_id: {hugging_face_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce091bff-59ba-455f-8eee-60b55613915e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_target_table_exists(target_table_name: str) -> None:\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {target_table_name} (\n",
    "            id INT NOT NULL,\n",
    "            text STRING NOT NULL,\n",
    "            label INT NOT NULL,\n",
    "            split STRING NOT NULL,\n",
    "            input_ids ARRAY<INT> NOT NULL,\n",
    "            full_token_count INT NOT NULL,\n",
    "            truncated_token_count INT NOT NULL,\n",
    "            CONSTRAINT pk_{target_table_name} PRIMARY KEY (id)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_append_table(\n",
    "    spark: SparkSession,\n",
    "    source_df: DataFrame,\n",
    "    target_table_name: str\n",
    ") -> None:\n",
    "    target_table = DeltaTable.forName(spark, target_table_name)\n",
    "    merge = (\n",
    "        target_table.alias(\"target\")\n",
    "        .merge(source_df.alias(\"source\"), \"source.id = target.id\")\n",
    "        .whenNotMatchedInsertAll()\n",
    "    )\n",
    "    merge.execute()\n",
    "\n",
    "\n",
    "@F.pandas_udf(returnType=T.IntegerType())\n",
    "def count_tokens_udf(\n",
    "    text_series_iterator: Iterator[pd.Series],\n",
    ") -> Iterator[pd.Series]:\n",
    "    # Tokenize the full text so we can analyze how many tokens it would \n",
    "    # include if we weren't truncating. We can't simply chop this off as\n",
    "    # we might not handle special tokens properly.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hugging_face_id)\n",
    "    for text_series in text_series_iterator:\n",
    "        texts = text_series.to_list()\n",
    "        tokenized = tokenizer(texts)\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        counts = [len(x) for x in input_ids]\n",
    "        yield pd.Series(counts)\n",
    "\n",
    "\n",
    "@F.pandas_udf(returnType=T.ArrayType(T.IntegerType()))\n",
    "def tokenize_and_truncate_udf(\n",
    "    text_series_iterator: Iterator[pd.Series],\n",
    ") -> Iterator[pd.Series]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hugging_face_id)\n",
    "    for text_series in text_series_iterator:\n",
    "        texts = text_series.to_list()\n",
    "        tokenized = tokenizer(texts, truncation=True)\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        yield pd.Series(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2033ae-f777-45aa-995d-3fb89a577c55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define the silver transformation"
    }
   },
   "outputs": [],
   "source": [
    "df_silver = (\n",
    "    spark.table(source_table_name)\n",
    "    .withColumn(\"input_ids\", tokenize_and_truncate_udf(F.col(\"text\")))\n",
    "    .withColumn(\"full_token_count\", count_tokens_udf(F.col(\"text\")))\n",
    "    .withColumn(\"truncated_token_count\", F.size(F.col(\"input_ids\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41231342-5901-4f26-8868-743c0cf5c73f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute the transformation"
    }
   },
   "outputs": [],
   "source": [
    "ensure_target_table_exists(target_table_name)\n",
    "merge_append_table(spark, df_silver, target_table_name)\n",
    "row_count = spark.table(target_table_name).count()\n",
    "print(f\"row_count: {row_count}\")\n",
    "display(spark.table(target_table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a62868-4ee3-448e-b035-a3c497d98bb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualize the token distribution"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"bnVtX2JpbnMgPSAyMCAgIyBDaG9vc2UgYXBwcm9wcmlhdGUgbnVtYmVyIG9mIGJpbnMKcmVzdWx0ID0gKAogICAgc3BhcmsudGFibGUodGFyZ2V0X3RhYmxlX25hbWUpCiAgICAuc2VsZWN0KEYuaGlzdG9ncmFtX251bWVyaWMoJ3Rva2VuX2NvdW50JywgRi5saXQobnVtX2JpbnMpKS5hbGlhcygiaGlzdCIpKQogICAgLnNlbGVjdChGLmlubGluZSgiaGlzdCIpKQopCmRpc3BsYXkocmVzdWx0KQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewb434df5\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewb434df5\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewb434df5\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewb434df5) SELECT `x`,SUM(`y`) `column_34285eaa155` FROM q GROUP BY `x`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewb434df5\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Token count histogram",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "x",
             "id": "column_34285eaa151"
            },
            "y": [
             {
              "column": "y",
              "id": "column_34285eaa155",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_34285eaa152": {
             "name": "y",
             "type": "column",
             "yAxis": 0
            },
            "column_34285eaa155": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "category"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "962feec6-8c00-436e-9e60-fc447a34a322",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 4.25,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "x",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "x",
           "type": "column"
          },
          {
           "alias": "column_34285eaa155",
           "args": [
            {
             "column": "y",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_bins = 20  # Choose appropriate number of bins\n",
    "result = (\n",
    "    spark.table(target_table_name)\n",
    "    .select(F.histogram_numeric(\"full_token_count\", F.lit(num_bins)).alias(\"hist\"))\n",
    "    .select(F.inline(\"hist\"))\n",
    ")\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_docs",
   "widgets": {
    "catalog_name": {
     "currentValue": "users",
     "nuid": "d94ade50-2a0e-49cb-bc81-9db064c73889",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "hugging_face_id": {
     "currentValue": "google-bert/bert-base-cased",
     "nuid": "303bd959-cce6-428c-8b83-331f21c63042",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "hugging_face_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "hugging_face_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "corey_abshire",
     "nuid": "d8cff1bd-95d6-4b56-b137-49c977a220ba",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
