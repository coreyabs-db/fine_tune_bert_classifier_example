{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70e12abf-f7f2-4541-9fac-287dbf2354ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load documents\n",
    "\n",
    "Read the documents from an external source into Databricks.\n",
    "\n",
    "In this case we are simply reading from a Hugging Face dataset, but \n",
    "you may be reading from a cloud storage bucket directly, or via Databricks Volumes,\n",
    "or from structured streaming or Lakeflow Pipelines sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bdc1ea0-2a12-4762-8efd-54b9f5e384d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports and setup"
    }
   },
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import SparkSession, DataFrame, Window, Row\n",
    "\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "import ray.train.huggingface.transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4edf4fad-80dc-4b0a-9bcf-a826197f89fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Task parameters"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"hf_datasets_cache\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "hf_datasets_cache = dbutils.widgets.get(\"hf_datasets_cache\")\n",
    "\n",
    "assert catalog_name, \"catalog_name is required\"\n",
    "assert schema_name, \"schema_name is required\"\n",
    "assert hf_datasets_cache, \"hf_datasets_cache is required\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = hf_datasets_cache\n",
    "\n",
    "target_table_name = \"yelp_reviews_bronze\"\n",
    "\n",
    "datasets.utils.logging.disable_progress_bar()\n",
    "\n",
    "print(f\"catalog_name: {catalog_name}\")\n",
    "print(f\"schema_name: {schema_name}\")\n",
    "print(f\"hf_datasets_cache: {hf_datasets_cache}\")\n",
    "print(f\"target_table_name: {target_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9a650b-319a-4198-900d-e3f99bc108f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper functions"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_target_table_exists(target_table_name: str) -> None:\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {target_table_name} (\n",
    "            id INT NOT NULL,\n",
    "            text STRING NOT NULL,\n",
    "            label INT NOT NULL,\n",
    "            split STRING NOT NULL,\n",
    "            CONSTRAINT pk_{target_table_name} PRIMARY KEY (id)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_split_as_dataframe(\n",
    "    dataset: datasets.Dataset,\n",
    "    split: str,\n",
    "    start_id: int = 0\n",
    ") -> DataFrame:\n",
    "    assert split in dataset.keys()\n",
    "    id_assignment = np.arange(dataset[split].num_rows) + start_id\n",
    "    return (\n",
    "        spark.createDataFrame(\n",
    "            dataset[split]\n",
    "            .add_column(\"id\", id_assignment),\n",
    "            schema=T.StructType([\n",
    "                T.StructField(\"id\", T.IntegerType()),\n",
    "                T.StructField(\"text\", T.StringType()),\n",
    "                T.StructField(\"label\", T.IntegerType()),\n",
    "            ])\n",
    "        )\n",
    "        .withColumn(\"split\", F.lit(split))\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_append_table(\n",
    "    spark: SparkSession,\n",
    "    source_df: DataFrame,\n",
    "    target_table_name: str\n",
    ") -> None:\n",
    "    target_table = DeltaTable.forName(spark, target_table_name)\n",
    "    merge = (\n",
    "        target_table.alias(\"target\")\n",
    "        .merge(source_df.alias(\"source\"), \"source.id = target.id\")\n",
    "        .whenNotMatchedInsertAll()\n",
    "    )\n",
    "    merge.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0864b527-46e0-47ac-9d4e-7b19efec227f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare dataframes"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "df_train = get_split_as_dataframe(dataset, \"train\", start_id=0)\n",
    "df_test = get_split_as_dataframe(dataset, \"test\", start_id=dataset[\"train\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caddcdc2-bfef-4b8f-8c53-e5e788f88038",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write the results"
    }
   },
   "outputs": [],
   "source": [
    "ensure_target_table_exists(target_table_name)\n",
    "merge_append_table(spark, df_train, target_table_name)\n",
    "merge_append_table(spark, df_test, target_table_name)\n",
    "row_count = spark.table(target_table_name).count()\n",
    "print(f\"row_count: {row_count}\")\n",
    "display(spark.table(target_table_name))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_docs",
   "widgets": {
    "catalog_name": {
     "currentValue": "users",
     "nuid": "81bbb76e-f9bb-44e2-9100-f9c50c4adc1e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "hf_datasets_cache": {
     "currentValue": "/Volumes/users/corey_abshire/yelp_cache_dir/",
     "nuid": "9f62cdb7-44f6-41f8-b85a-15154ab95f08",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "hf_datasets_cache",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "hf_datasets_cache",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "corey_abshire",
     "nuid": "2d0289f6-4210-4b28-99c4-f3b2e184ad1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
